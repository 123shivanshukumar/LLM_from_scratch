# GPT2 model structure

## Tokenisation
- tikToken by GPT, sentecePiece by google (subword level tokenisation)
- 
(Generatively Pretrained Transformer )
- follows decoder only transformer architecture
- **GELU** smoother RELU : to boost performance approxiamation using tanh
- 